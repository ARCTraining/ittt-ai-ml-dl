{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Session 3: Text analysis approaches\n",
    "\n",
    "\\#\\#\\# __DRAFT__ \\#\\#\\#\n",
    "\n",
    "Text analysis is a classic computational and data science problem, \n",
    "\n",
    "Compared with regression and classification approaches on continuous and categorical dataset taking text data and deriving distinct insights is a far more complicated task. Text data and especially free text (text fields in sentence form) is typically classed as a form of unstructured data because of the various nuances introduced by languages.\n",
    "\n",
    "With the ever increasing computational power has come a side-by-side improvements in approaches to text analysis. \n",
    "\n",
    "The idea of topic modelling, identifying abstract 'topics' within a collection of documents (corpus) using statistical models, was first described in 1998, with probabilistic latent semantic analysis (PLSA) outlined in 1999 and latent Dirichlet allocation (LDA) developed in [2002](http://jmlr.csail.mit.edu/papers/v3/blei03a.html). LDA has become one of the most commonly used topic modelling approaches since although many extensions of LDA have since been proposed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gensim\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash \n",
    "\n",
    "if test -f data/subset_twcs.csv; then\n",
    "    echo \"Data file exists\"\n",
    "else \n",
    "    tar xzf data/subtwcs.tar.gz\n",
    "fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the dataset\n",
    "\n",
    "twitter_data = pd.read_csv('data/subset_twcs.csv')\n",
    "\n",
    "twitter_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a sub dataframe for just data that was inbound\n",
    "inbound_dat = twitter_data[twitter_data.inbound == True]\n",
    "\n",
    "inbound_dat.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inbound_dat.author_id.unique().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join together all inbound tweets from the same user\n",
    "\n",
    "user_tweets = inbound_dat.groupby('author_id')['text'].apply(lambda x: ','.join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_tweets = user_tweets.reset_index()\n",
    "\n",
    "user_tweets.text[:10].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "Pre processing is a crucial step in any text analytics project. Text data on its own is very difficult for machines to understand and therefore it requires cleaning and preparing before building models. This often involves a number of steps such as:\n",
    "- Tokenisation, converting a long string of words into a list of individual words i.e. \"the cat sat on the mat\" -> [\"the\", \"cat\", \"sat\", \"on\", \"the\", \"mat\"]\n",
    "- Noise removal, most commonly removing punctuation or things like hyperlinks or emojis\n",
    "- Stopword removal, removing common words that don't contain information such as the, and, or, a \n",
    "- Stemming or lemming, this is the process of reverting words to their root either by chopping off suffixes (stemming) or reverting to word lemma (lemming)\n",
    "- Normalisation, commonly this means converting all words to lower or uppercase\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.parsing.preprocessing import preprocess_string, strip_tags, strip_punctuation, strip_numeric, remove_stopwords\n",
    "\n",
    "def basic_preprocess(list_of_strings):\n",
    "    \"\"\"\n",
    "    A basic function that takes a list of strings and runs some basic\n",
    "    gensim preprocessing to tokenise each string.\n",
    "    \n",
    "    Operations:\n",
    "        - convert to lowercase\n",
    "        - remove html tags\n",
    "        - remove punctuation\n",
    "        - remove numbers\n",
    "    \n",
    "    Outputs a list of lists\n",
    "    \"\"\"\n",
    "    \n",
    "    CUSTOM_FILTERS = [lambda x: x.lower(), strip_tags, strip_punctuation, strip_numeric, remove_stopwords]\n",
    "\n",
    "    preproc_text = [preprocess_string(doc, CUSTOM_FILTERS) for doc in list_of_strings]\n",
    "    \n",
    "    return preproc_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what are stop words?\n",
    "\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "\n",
    "print(STOPWORDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def remove_twitterisms(list_of_strings):\n",
    "    \"\"\"\n",
    "    Some regular expression statements to remove twitter-isms\n",
    "    \n",
    "    Operations:\n",
    "        - remove links\n",
    "        - remove @tag\n",
    "        - remove #tag\n",
    "        \n",
    "    Returns list of strings with the above removed\n",
    "    \"\"\"\n",
    "    \n",
    "    # removing some standard twitter-isms\n",
    "\n",
    "    list_of_strings = [re.sub(r\"http\\S+\", \"\", doc) for doc in list_of_strings]\n",
    "\n",
    "    list_of_strings = [re.sub(r\"@\\S+\", \"\", doc) for doc in list_of_strings]\n",
    "\n",
    "    list_of_strings = [re.sub(r\"#\\S+\", \"\", doc) for doc in list_of_strings]\n",
    "    \n",
    "    return list_of_strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing emojis\n",
    "# taken from https://gist.github.com/slowkow/7a7f61f495e3dbb7e3d767f97bd7304b#gistcomment-3315605\n",
    "\n",
    "def remove_emoji(string):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                               u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
    "                               u\"\\U00002702-\\U000027B0\"\n",
    "                               u\"\\U00002702-\\U000027B0\"\n",
    "                               u\"\\U000024C2-\\U0001F251\"\n",
    "                               u\"\\U0001f926-\\U0001f937\"\n",
    "                               u\"\\U00010000-\\U0010ffff\"\n",
    "                               u\"\\u2640-\\u2642\"\n",
    "                               u\"\\u2600-\\u2B55\"\n",
    "                               u\"\\u200d\"\n",
    "                               u\"\\u23cf\"\n",
    "                               u\"\\u23e9\"\n",
    "                               u\"\\u231a\"\n",
    "                               u\"\\ufe0f\"  # dingbats\n",
    "                               u\"\\u3030\"\n",
    "                               \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.phrases import Phrases\n",
    "\n",
    "def n_gram(tokens):\n",
    "    \"\"\"Identifies common two/three word phrases using gensim module.\"\"\"\n",
    "    # Add bigrams and trigrams to docs (only ones that appear 10 times or more).\n",
    "    # includes threshold kwarg (threshold score required by bigram)\n",
    "    bigram = Phrases(tokens, min_count=10, threshold=100)\n",
    "    trigram = Phrases(bigram[tokens], threshold = 100)\n",
    "\n",
    "    for idx, val in enumerate(tokens):\n",
    "        for token in bigram[tokens[idx]]:\n",
    "            if '_' in token:\n",
    "                if token not in tokens[idx]:\n",
    "                    # Token is a bigram, add to document.bigram\n",
    "                    tokens[idx].append(token)\n",
    "        for token in trigram[tokens[idx]]:\n",
    "            if '_' in token:\n",
    "                if token not in tokens[idx]:\n",
    "                    # Token is a trigram, add to document.\n",
    "                    tokens[idx].append(token)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def test_basic_proprocess():\n",
    "    \n",
    "    # just to randomise tests\n",
    "    randomize = random.randrange(0,3)\n",
    "    \n",
    "    test_doc = [\n",
    "        \"THE CAT SAT ON THE MAT!\",\n",
    "        \"<h1> hello world?</h1>\",\n",
    "        \"wElL WeLl We?L, wHaT <strong>HaVe wE</strong>hERe\"\n",
    "    ]\n",
    "    \n",
    "    test_output = basic_preprocess(test_doc)\n",
    "    \n",
    "    # test if func returns list of lists\n",
    "    assert isinstance(test_output[randomize], list)\n",
    "    # test if all items \n",
    "    assert [x.islower() for x in test_output[randomize]] == [True] * len(test_output[randomize])\n",
    "    # test if html tags are removed\n",
    "    assert \"<h1>\" not in test_output[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# next we implement the preprocessing steps\n",
    "\n",
    "preprocessed_corpus = remove_twitterisms(user_tweets.text.tolist())\n",
    "\n",
    "preprocessed_corpus = [remove_emoji(doc) for doc in preprocessed_corpus]\n",
    "\n",
    "preprocessed_corpus = basic_preprocess(preprocessed_corpus)\n",
    "\n",
    "preprocessed_corpus = n_gram(preprocessed_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets compare the original strings to the preprocessed strings\n",
    "for orig, proc in zip(user_tweets.text.tolist()[:10], preprocessed_corpus[:10]):\n",
    "    \n",
    "    print(orig)\n",
    "    print(proc)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a bag of words corpus and dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from gensim.corpora import Dictionary\n",
    "\n",
    "def bag_of_word_processing(corpus_of_tokens, lower_extreme, upper_extreme):\n",
    "    \"\"\"\n",
    "    Take the list of tokens and convert them into a bag-of-words (BoW) format.\n",
    "\n",
    "    Extended description of function.\n",
    "\n",
    "    :param list of lists corpus_of_tokens: a list of strings produced during preprocessing representing all documents in corpus\n",
    "    :param int lower_extreme: Description of arg2.\n",
    "    :param float upper_extreme: the upper extreme filter limit, words are excluded if they occur in more documents than the proportion specified here\n",
    "    :return: gensim.corpora.dictionary.Dictionary object \n",
    "    :return: list representing BoW corpus\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    # Create a dictionary representation of the documents.\n",
    "    # gensim Dictionary function creates tokens -> tokenID dict\n",
    "    dictionary = Dictionary(corpus_of_tokens)\n",
    "    print('Number of unique words in initital documents:', len(dictionary))\n",
    "\n",
    "    org_dict = len(dictionary)\n",
    "\n",
    "    # Filter out words that occur less than 10 documents, or more than 70% of the documents.\n",
    "    dictionary.filter_extremes(no_below=lower_extreme, no_above=upper_extreme)\n",
    "    print('Number of unique words after removing rare and common words:', len(dictionary))\n",
    "\n",
    "    filt_dict = len(dictionary)\n",
    "\n",
    "    print('Token reduction of: ' + str((1-filt_dict/org_dict)*100)+'%')\n",
    "\n",
    "    # transform to bag of words\n",
    "    corpus = [dictionary.doc2bow(doc) for doc in corpus_of_tokens]\n",
    "    print('Number of unique tokens: %d' % len(dictionary))\n",
    "    print('Number of documents: %d' % len(corpus))\n",
    "\n",
    "    # output on document length\n",
    "    print('Average number of words per document before BoW transform: ', np.mean([len(item) for item in preprocessed_corpus]))\n",
    "    print('Average number of words per BoW document: ',np.mean([len(corpus[i]) for i in range(len(corpus))]))\n",
    "\n",
    "    return dictionary, corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "working_dict, working_corpus = bag_of_word_processing(preprocessed_corpus, 10, 0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the topic model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import CoherenceModel, ldamulticore\n",
    "\n",
    "def calculate_scores(dictionary, corpus,  texts, limit, start=2, step=3):\n",
    "    \"\"\"\n",
    "    Compute c_v coherence for a wide range of topic numbers.\n",
    "    Adapted from https://www.machinelearningplus.com/nlp/topic-modeling-gensim-python/\n",
    "    Parameters:\n",
    "    ----------\n",
    "    dictionary : Gensim dictionary\n",
    "    corpus : Gensim corpus\n",
    "    method = c_v or u_mass\n",
    "    texts : List of input texts (doc_clean)\n",
    "    limit : Max num of topics\n",
    "    Returns:\n",
    "    -------\n",
    "    coherence_values : Coherence values corresponding to the LDA model with respective number of topics\n",
    "    graphical outputs\n",
    "    \"\"\"\n",
    "    coherence_dict = dict()\n",
    "\n",
    "    for num_topics in range(start, limit, step):\n",
    "        model = ldamulticore.LdaMulticore(corpus=corpus, num_topics=num_topics, id2word=dictionary, workers=2)\n",
    "        coherencemodel1 = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v')\n",
    "        coherence_dict[num_topics] = coherencemodel1.get_coherence()\n",
    "\n",
    "    coherence_df = pd.DataFrame(pd.Series(coherence_dict)).reset_index()\n",
    "\n",
    "    coherence_df.columns = ['Num_topics','Coherence_score']\n",
    "\n",
    "    # Show graph\n",
    "    fig, ax = plt.subplots(figsize=(12,10))\n",
    "    ax.plot(coherence_df['Num_topics'], coherence_df['Coherence_score'])\n",
    "    ax.set_xlabel(\"No. of topics\", fontweight='bold')\n",
    "    ax.set_ylabel(\"Cv Coherence score\", fontweight='bold')\n",
    "    ax.axvline(coherence_df[coherence_df['Coherence_score'] == coherence_df['Coherence_score'].max()]['Num_topics'].tolist(), color='red')\n",
    "\n",
    "    return coherence_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_scores(working_dict, working_corpus, preprocessed_corpus, limit=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
