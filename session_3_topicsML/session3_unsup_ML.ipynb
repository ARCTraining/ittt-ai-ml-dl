{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    },
    "colab": {
      "name": "session3_unsup_ML.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sparrow0hawk/ittt-ai-ml-dl/blob/session3-AC/session_3_topicsML/session3_unsup_ML.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FAB9STo8Ba9o",
        "colab_type": "text"
      },
      "source": [
        "<img src=https://raw.githubusercontent.com/ARCLeeds/arcleeds.github.io/master/assets/img/lighterblueText_wLogo_m2.1.png alt=\"Research Computing logo\" style=\"width:900px;\">\n",
        "\n",
        "# IT TechTalk - Session 2: Text analysis approaches\n",
        "\n",
        "## Agenda\n",
        "\n",
        "- [Introduction](#Introduction)\n",
        "- [Preprocessing](#Preprocessing)\n",
        "- [Build a Bag of words corpus](#Building-a-bag-of-words-corpus-and-dictionary)\n",
        "- [Building a topic model](#Building-the-topic-model)\n",
        "\n",
        "## Introduction\n",
        "\n",
        "Text analysis is a classic computational and data science problem.\n",
        "\n",
        "![NLP](https://deeplearninganalytics.org/wp-content/uploads/2019/04/nlp.png)\n",
        "\n",
        "Compared with regression and classification approaches on continuous and categorical dataset taking text data and deriving distinct insights is a far more complicated task. Text data and especially free text (text fields in sentence form) is typically classed as a form of unstructured data because of the various nuances introduced by languages.\n",
        "\n",
        "With the ever increasing computational power has come a side-by-side improvements in approaches to text analysis. \n",
        "\n",
        "There are a number of different approaches to text analysis such as sentiment analysis, machine translation, information retrieval and much more. In this talk we'll focus specifically on **topic modelling**. An unsupervised statistical approach for identifying abstract 'topics' from within a collection of documents (corpus).\n",
        "\n",
        "We'll look specifically at latent Dirichlet allocation (LDA), a topic modelling approach developed in [2002](http://jmlr.csail.mit.edu/papers/v3/blei03a.html). LDA has become one of the most commonly used topic modelling approaches and many extensions of LDA have since been proposed.\n",
        "\n",
        "We'll use financial complaints data from the [US Consumer Financial Protection Bureau](https://www.consumerfinance.gov/data-research/consumer-complaints/#download-the-data) for this example."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GQ-x2m_KVlLs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# line wrapping for colab https://stackoverflow.com/questions/58890109/line-wrapping-in-collaboratory-google-results\n",
        "from IPython.display import HTML, display\n",
        "\n",
        "def set_css():\n",
        "  display(HTML('''\n",
        "  <style>\n",
        "    pre {\n",
        "        white-space: pre-wrap;\n",
        "    }\n",
        "  </style>\n",
        "  '''))\n",
        "get_ipython().events.register('pre_run_cell', set_css)"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pCMhzMb7Ba9s",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 607
        },
        "outputId": "6b5b8b92-2ba4-44ee-e6b1-bf0b8c2fde71"
      },
      "source": [
        "!pip install gensim==3.8.0\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import gensim\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Collecting gensim==3.8.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/40/3d/89b27573f56abcd1b8c9598b240f53c45a3c79aa0924a24588e99716043b/gensim-3.8.0-cp36-cp36m-manylinux1_x86_64.whl (24.2MB)\n",
            "\u001b[K     |████████████████████████████████| 24.2MB 1.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.6/dist-packages (from gensim==3.8.0) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.6/dist-packages (from gensim==3.8.0) (1.18.5)\n",
            "Requirement already satisfied: smart-open>=1.7.0 in /usr/local/lib/python3.6/dist-packages (from gensim==3.8.0) (2.1.0)\n",
            "Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from gensim==3.8.0) (1.15.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.7.0->gensim==3.8.0) (2.23.0)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.7.0->gensim==3.8.0) (1.14.37)\n",
            "Requirement already satisfied: boto in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.7.0->gensim==3.8.0) (2.49.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.7.0->gensim==3.8.0) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.7.0->gensim==3.8.0) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.7.0->gensim==3.8.0) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.7.0->gensim==3.8.0) (2020.6.20)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.7.0->gensim==3.8.0) (0.3.3)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.7.0->gensim==3.8.0) (0.10.0)\n",
            "Requirement already satisfied: botocore<1.18.0,>=1.17.37 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.7.0->gensim==3.8.0) (1.17.37)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.18.0,>=1.17.37->boto3->smart-open>=1.7.0->gensim==3.8.0) (0.15.2)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.18.0,>=1.17.37->boto3->smart-open>=1.7.0->gensim==3.8.0) (2.8.1)\n",
            "Installing collected packages: gensim\n",
            "  Found existing installation: gensim 3.6.0\n",
            "    Uninstalling gensim-3.6.0:\n",
            "      Successfully uninstalled gensim-3.6.0\n",
            "Successfully installed gensim-3.8.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xdx29oJTBa94",
        "colab_type": "text"
      },
      "source": [
        "We add some shell scripting here to create a data directory and download and unzip our data file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TTCz86CWBa99",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "outputId": "4e0c129d-3754-4bb7-b490-1827c0236293"
      },
      "source": [
        "%%bash \n",
        "\n",
        "if [ -d data/ ]; then\n",
        "    echo \"Data directory exists\"\n",
        "else\n",
        "    mkdir data\n",
        "fi\n",
        "\n",
        "if test -f data/complaints.csv; then\n",
        "    echo \"Data file exists\"\n",
        "else \n",
        "    curl -LO http://files.consumerfinance.gov/ccdb/complaints.csv.zip; mv complaints.csv.zip data/ ;unzip data/complaints.csv.zip -d data/\n",
        "fi"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Archive:  data/complaints.csv.zip\n",
            "  inflating: data/complaints.csv     \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r100   183  100   183    0     0   1236      0 --:--:-- --:--:-- --:--:--  1236\n",
            "\r 45  263M   45  119M    0     0   126M      0  0:00:02 --:--:--  0:00:02  126M\r100  263M  100  263M    0     0   153M      0  0:00:01  0:00:01 --:--:--  186M\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GZHBFhkCBa-J",
        "colab_type": "text"
      },
      "source": [
        "Next we load this dataset into a pandas DataFrame python object. This is alot like a spreadsheet and allows for easy manipulation of columns and rows."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i2D7Q7LOBa-L",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 695
        },
        "outputId": "6051e638-8a55-4c16-a08c-6198ff1a82b3"
      },
      "source": [
        "# import the dataset\n",
        "# for demo purposes we'll use a subset of the data 5% of total\n",
        "ticket_data = pd.read_csv('data/complaints.csv')#.sample(frac=0.2, random_state=42)\n",
        "\n",
        "ticket_data.dropna(subset=[\"Consumer complaint narrative\"], inplace=True)\n",
        "\n",
        "print(ticket_data.shape)\n",
        "\n",
        "ticket_data.head()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "(578251, 18)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Date received</th>\n",
              "      <th>Product</th>\n",
              "      <th>Sub-product</th>\n",
              "      <th>Issue</th>\n",
              "      <th>Sub-issue</th>\n",
              "      <th>Consumer complaint narrative</th>\n",
              "      <th>Company public response</th>\n",
              "      <th>Company</th>\n",
              "      <th>State</th>\n",
              "      <th>ZIP code</th>\n",
              "      <th>Tags</th>\n",
              "      <th>Consumer consent provided?</th>\n",
              "      <th>Submitted via</th>\n",
              "      <th>Date sent to company</th>\n",
              "      <th>Company response to consumer</th>\n",
              "      <th>Timely response?</th>\n",
              "      <th>Consumer disputed?</th>\n",
              "      <th>Complaint ID</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2019-09-24</td>\n",
              "      <td>Debt collection</td>\n",
              "      <td>I do not know</td>\n",
              "      <td>Attempts to collect debt not owed</td>\n",
              "      <td>Debt is not yours</td>\n",
              "      <td>transworld systems inc. \\nis trying to collect...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>TRANSWORLD SYSTEMS INC</td>\n",
              "      <td>FL</td>\n",
              "      <td>335XX</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Consent provided</td>\n",
              "      <td>Web</td>\n",
              "      <td>2019-09-24</td>\n",
              "      <td>Closed with explanation</td>\n",
              "      <td>Yes</td>\n",
              "      <td>NaN</td>\n",
              "      <td>3384392</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2019-10-25</td>\n",
              "      <td>Credit reporting, credit repair services, or o...</td>\n",
              "      <td>Credit reporting</td>\n",
              "      <td>Incorrect information on your report</td>\n",
              "      <td>Information belongs to someone else</td>\n",
              "      <td>I would like to request the suppression of the...</td>\n",
              "      <td>Company has responded to the consumer and the ...</td>\n",
              "      <td>TRANSUNION INTERMEDIATE HOLDINGS, INC.</td>\n",
              "      <td>CA</td>\n",
              "      <td>937XX</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Consent provided</td>\n",
              "      <td>Web</td>\n",
              "      <td>2019-10-25</td>\n",
              "      <td>Closed with explanation</td>\n",
              "      <td>Yes</td>\n",
              "      <td>NaN</td>\n",
              "      <td>3417821</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2019-11-08</td>\n",
              "      <td>Debt collection</td>\n",
              "      <td>I do not know</td>\n",
              "      <td>Communication tactics</td>\n",
              "      <td>Frequent or repeated calls</td>\n",
              "      <td>Over the past 2 weeks, I have been receiving e...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Diversified Consultants, Inc.</td>\n",
              "      <td>NC</td>\n",
              "      <td>275XX</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Consent provided</td>\n",
              "      <td>Web</td>\n",
              "      <td>2019-11-08</td>\n",
              "      <td>Closed with explanation</td>\n",
              "      <td>Yes</td>\n",
              "      <td>NaN</td>\n",
              "      <td>3433198</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>2019-03-05</td>\n",
              "      <td>Mortgage</td>\n",
              "      <td>Conventional home mortgage</td>\n",
              "      <td>Trouble during payment process</td>\n",
              "      <td>NaN</td>\n",
              "      <td>The escrow unit of my mortgage servicing compa...</td>\n",
              "      <td>Company believes complaint represents an oppor...</td>\n",
              "      <td>Ditech Financial LLC</td>\n",
              "      <td>DC</td>\n",
              "      <td>200XX</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Consent provided</td>\n",
              "      <td>Web</td>\n",
              "      <td>2019-03-05</td>\n",
              "      <td>Closed with non-monetary relief</td>\n",
              "      <td>Yes</td>\n",
              "      <td>NaN</td>\n",
              "      <td>3170261</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>2019-09-08</td>\n",
              "      <td>Money transfer, virtual currency, or money ser...</td>\n",
              "      <td>Domestic (US) money transfer</td>\n",
              "      <td>Fraud or scam</td>\n",
              "      <td>NaN</td>\n",
              "      <td>I was sold access to an event digitally, of wh...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Paypal Holdings, Inc</td>\n",
              "      <td>RI</td>\n",
              "      <td>029XX</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Consent provided</td>\n",
              "      <td>Web</td>\n",
              "      <td>2019-09-08</td>\n",
              "      <td>Closed with explanation</td>\n",
              "      <td>Yes</td>\n",
              "      <td>NaN</td>\n",
              "      <td>3366475</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  Date received  ... Complaint ID\n",
              "0    2019-09-24  ...      3384392\n",
              "2    2019-10-25  ...      3417821\n",
              "3    2019-11-08  ...      3433198\n",
              "5    2019-03-05  ...      3170261\n",
              "8    2019-09-08  ...      3366475\n",
              "\n",
              "[5 rows x 18 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bJQ_dPJITwP1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 347
        },
        "outputId": "d508bfe6-bedc-4234-89de-6120ab52fc57"
      },
      "source": [
        "ticket_data.Product.value_counts()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Credit reporting, credit repair services, or other personal consumer reports    193159\n",
              "Debt collection                                                                 119378\n",
              "Mortgage                                                                         67553\n",
              "Credit card or prepaid card                                                      40701\n",
              "Credit reporting                                                                 31588\n",
              "Student loan                                                                     26682\n",
              "Checking or savings account                                                      23487\n",
              "Credit card                                                                      18838\n",
              "Bank account or service                                                          14885\n",
              "Vehicle loan or lease                                                             9913\n",
              "Money transfer, virtual currency, or money service                                9883\n",
              "Consumer Loan                                                                     9473\n",
              "Payday loan, title loan, or personal loan                                         7710\n",
              "Payday loan                                                                       1746\n",
              "Money transfers                                                                   1497\n",
              "Prepaid card                                                                      1450\n",
              "Other financial service                                                            292\n",
              "Virtual currency                                                                    16\n",
              "Name: Product, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6dA-pDfbS1Jz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 364
        },
        "outputId": "02cd8108-14ff-4a95-ee20-893cf80f7ca8"
      },
      "source": [
        "ticket_data.groupby('Product')['Consumer complaint narrative'].apply(lambda x: np.mean(len(x)))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Product\n",
              "Bank account or service                                                          14885.0\n",
              "Checking or savings account                                                      23487.0\n",
              "Consumer Loan                                                                     9473.0\n",
              "Credit card                                                                      18838.0\n",
              "Credit card or prepaid card                                                      40701.0\n",
              "Credit reporting                                                                 31588.0\n",
              "Credit reporting, credit repair services, or other personal consumer reports    193159.0\n",
              "Debt collection                                                                 119378.0\n",
              "Money transfer, virtual currency, or money service                                9883.0\n",
              "Money transfers                                                                   1497.0\n",
              "Mortgage                                                                         67553.0\n",
              "Other financial service                                                            292.0\n",
              "Payday loan                                                                       1746.0\n",
              "Payday loan, title loan, or personal loan                                         7710.0\n",
              "Prepaid card                                                                      1450.0\n",
              "Student loan                                                                     26682.0\n",
              "Vehicle loan or lease                                                             9913.0\n",
              "Virtual currency                                                                    16.0\n",
              "Name: Consumer complaint narrative, dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VWwtnVj_TWZq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "849434c0-9a9e-4ef6-ebbd-a08575d7317e"
      },
      "source": [
        "ticket_data = ticket_data[ticket_data['Product'] == 'Credit card']"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NG9LyS2_Ba-b",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "104d261d-d467-4bf5-c537-786f976ec1ad"
      },
      "source": [
        "# lets peak and look what this looks like\n",
        "\n",
        "ticket_data['Consumer complaint narrative'][:5].tolist()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['I was stupid enough to charge some items at MACY \\'S on my Macy \\'s credit card over XXXX. I was unable to log into my account online to make the payment because Macy \\'s had updated the system and I simply did n\\'t have the patience to navigate their irritating and obscure system. However, I called Macy \\'s and paid my bill for $ XXXX a day or two late - was assured that there would be no late charges because I was a customer in good standing, yada yada, and received a confirmation number that I had paid the bill in full. \\n\\nI then received a bill for {$2.00} in interest. OK, I agree - I had n\\'t paid the {$140.00} on time, regardless of the reason. Whatever. So I paid that {$2.00} bill immediately, by check, and left the country for an extended absence - in complete confidence that I had paid all my bills in full. \\n\\nImagine my surprise when I returned to the US and found that Macy \\'s had received and cashed my check, but charged me a further {$2.00} anyway as a minimum interest charge on the interest charge that had been paid in full, well before the due date. Oh, and then added a further minimum {$2.00} charge for a total of {$4.00}, due on XXXX. \\n\\nI called Macy \\'s right away and spoke to one XXXX, who did n\\'t seem to have a clue why I was irritated, but eventually - very grudgingly and with full implication that I was a total deadbeat - agreed that she would \" waive \\'\\' the {$4.00}. \\n\\nImagine my FURTHER surprise when I received a FURTHER email from Macy \\'s about my \" account balance. \\'\\' I immediately called, went through endless voice prompts, and finally got through to a live human being who said in hushed tones that she \\'d have to put me through to another department. \\n\\nSure enough, I got the \" this is an attempt to collect a debt \\'\\' speech. The glorious XXXX, according to the faultlessly polite CS rep who answered, had failed to properly credit my account. He agreed that he could correct it and that my account would update eventually to show a {$0.00} balance. However, he was unable to reassure me that this, um, \" delinquency \\'\\' had not been reported to the credit bureaux that make a darn good living off selling my info, and he could not send me a letter saying that Macy \\'s had made a mistake. Given that the glorious XXXX had also assured me that I had a {$0.00} balance on my Macy \\'s charge card, forgive me if I \\'m not confident that I do in fact now have a {$0.00} balance. \\n\\nI do not give a flying whatever about my credit as I am old and cranky and do n\\'t need any credit. I DO care that Macy \\'s total incompetence and the Macy \\'s system that is set up to fail, to their benefit and to consumers \\' detriment, affects kids and other people who DO need good credit.',\n",
              " \"XXXX XXXX, 2016 To Whom It May Concern : I 'm outraged US Bank debited my account {$570.00} without my authorization after a credit card payment! On XXXX XXXX, I completed an external transfer for {$300.00} and {$270.00} respectively to my XXXX US Bank credit cards. However I noticed my checking account had double posts from US Bank totaling {$1100.00}. Immediately I called and spoke to a representative and she confirmed there had been a mistake on their system and expect a refund in four days. I 've called three more times and spoke to several representatives that stated my money can be held for 17 days until returned! I have also left voice-mails to the special payment research department and have yet to receive a phone call back. This is unacceptable for my money to be held while US Bank makes interest for over a week! My funds need to be returned to my bank account immediately with interest! I 'm looking for a fast resolution and funds returned as soon as possible. \\nXXXX XXXX XXXX\",\n",
              " 'I have rented a car XXXX and paid the rental amount with my credit card. After that, i have received almost {$1500.00} bill from the same car rental company for damages to the car which i was not agreed. The rental car company had forged my signature on a paper to charge my credit card and bank of america has denied my dispute even thought the signature for forged ...',\n",
              " 'I opened up this credit card in 2008. I have never been late on a payment. The majority of this debt was consolidated date because I had a 7.79 percent rate on this card. I always paid double the amount of the min payment. This last year I have been working hard to get other debt with higher interest rates paid off so I reduced my payment to the minimum on this capital one card. My payment was {$250.00}. I went out of town for a period of time in XX/XX/XXXX and XX/XX/XXXX and my bank makes the payments automatically. Unbeknownst to me the min payment went up {$3.00}. So in XX/XX/XXXX they received {$250.00} instead of {$250.00}. In XX/XX/XXXX ( I was out of town and did not know it was {$3.00} short ) They received {$250.00} instead of {$250.00}. I got back and realized they have raised my APR from 7.79 % to 24.99 % over a shortage of {$3.00}. I called and asked them to reduce it back considering my history of never being late and simply because it was not ethical to raise the APR to 24.99 %. I will never get out of debt paying this rate. I spoke to XXXX different people over a period of 3 hours. The last person XXXX ID # XXXX basically said she knew and understood what had happened and she knows what I want ... to have the rate lowered back to the original..but she WAS NOT DOING IT. It is not a matter of the dollar amount agreeing that {$3.00} was a meager amount to be short but rather that I was short twice and that it was not worth saving a good customer over {$3.00}. She was a horrible customer service rep. I wonder if Capital One really does not value good customers as she portrays. I want the interest rate reduced back to what I deserve having always paid on time. I was not \" Late \\'\\' on a payment I was short {$3.00}. That does not warrant raping me financially. I owe Capital One {$15000.00}. They stand to make a lot of money over a {$3.00} mistake and this should not happen much less be legal business practice.',\n",
              " \"Earlier this month, I set up an electronic payment from my primary checking account to make a payment on a Citi Visa card prior to the due date. I then received an email letting me know I had a secure message online ( after the due date ). I logged into my account and found a letter ( PDF ) stating that the payment was never made. In this letter, Citi had my correct routing number but my checking account number was missing XXXX digit. To correct the issue, I first tried to re-enter my correct checking account information, which Citi would n't accept since they said they already had this checking account on file. I then tried to delete the checking account information to re-enter it, but I was locked out from making changes to my banking information online. So I then contacted Citi again asking them how I should proceed. I then received another email letting me know that there was a secure message online. But when I logged into my account this time there was no letter ( PDF ) or electronic message. I also noticed that I was assessed a {$25.00} fee ( for a returned check ). And, just now, I received an email letting me know that Citi reviewed this account and lowered my credit limit by more than {$1000.00} to the point that this card is essentially maxed out. \\n\\nI feel like I am either being punished for noting an error on the part of Citi that they are trying to cover up. Or that Citi used this error ( or prompted this error ) to create a situation where I would unknowingly be late on a payment just to lower my credit limit and thus negatively impact my credit score.\"]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bsk9Tg0zBa-j",
        "colab_type": "text"
      },
      "source": [
        "## Preprocessing\n",
        "\n",
        "Pre processing is a crucial step in any text analytics project. Text data on its own is very difficult for machines to understand and therefore it requires cleaning and preparing before building models. This often involves a number of steps such as:\n",
        "- Tokenisation, converting a long string of words into a list of individual words i.e. \"the cat sat on the mat\" -> [\"the\", \"cat\", \"sat\", \"on\", \"the\", \"mat\"]\n",
        "- Noise removal, most commonly removing punctuation or things like hyperlinks or emojis\n",
        "- Stopword removal, removing common words that don't contain information such as the, and, or, a \n",
        "- Stemming or lemming, this is the process of reverting words to their root either by chopping off suffixes (stemming) or reverting to word lemma (lemming)\n",
        "- Normalisation, commonly this means converting all words to lower or uppercase\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5uabW4WGBa-k",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "cb1138dd-291c-477a-e414-540f671e0fe3"
      },
      "source": [
        "# lets slice out the text data from our dataframe\n",
        "subsample_text = ticket_data['Consumer complaint narrative'].tolist()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BQXe9d2FBa-r",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "7f2e860d-53fa-4247-c6c6-37fed735e369"
      },
      "source": [
        "from gensim.parsing.preprocessing import preprocess_string, strip_tags, strip_punctuation, strip_numeric, remove_stopwords, strip_short, stem_text\n",
        "\n",
        "def basic_preprocess(list_of_strings):\n",
        "    \"\"\"\n",
        "    A basic function that takes a list of strings and runs some basic\n",
        "    gensim preprocessing to tokenise each string.\n",
        "    \n",
        "    Operations:\n",
        "        - convert to lowercase\n",
        "        - remove html tags\n",
        "        - remove punctuation\n",
        "        - remove numbers\n",
        "        - remove short tokens (less than 3)\n",
        "    \n",
        "    Outputs a list of lists\n",
        "    \"\"\"\n",
        "    \n",
        "    CUSTOM_FILTERS = [lambda x: x.lower(), strip_tags, strip_punctuation, strip_numeric, remove_stopwords, strip_short]\n",
        "\n",
        "    preproc_text = [preprocess_string(doc, CUSTOM_FILTERS) for doc in list_of_strings]\n",
        "    \n",
        "    return preproc_text"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9fem9F6PBa-x",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 416
        },
        "outputId": "0fac7ae5-740e-40cb-89cb-08c929fa221c"
      },
      "source": [
        "# what are stop words?\n",
        "\n",
        "from gensim.parsing.preprocessing import STOPWORDS\n",
        "\n",
        "print(STOPWORDS)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "frozenset({'give', 'may', 'everyone', 'nor', 'as', 'across', 'they', 'us', 'mostly', 'these', 'me', 'empty', 'although', 'now', 'alone', 'during', 'sincere', 'anywhere', 'please', 'another', 'been', 'front', 'nothing', 'do', 'eight', 'too', 'against', 'three', 'thru', 'some', 'herself', 'elsewhere', 'if', 'once', 'through', 'four', 'move', 'seeming', 'further', 'for', 'didn', 'herein', 'each', 'my', 'the', 'in', 'describe', 'make', 'amongst', 'least', 'whither', 'very', 'couldnt', 'from', 'their', 'beforehand', 'it', 'interest', 'up', 'hereby', 'since', 'about', 'can', 'which', 'over', 'only', 'here', 'upon', 'whenever', 'becomes', 'whom', 'him', 'an', 'still', 'thereupon', 'does', 'moreover', 'don', 'etc', 'always', 'hereupon', 'again', 'done', 'also', 'perhaps', 'most', 'third', 'often', 'wherever', 'find', 'regarding', 'though', 'forty', 'mine', 'amoungst', 'her', 'this', 'cannot', 'between', 'fire', 'part', 'yourselves', 'bill', 'somehow', 'fifty', 'full', 'sometime', 'when', 'keep', 'of', 'same', 'con', 'or', 'himself', 'hasnt', 'onto', 'among', 'while', 'back', 'mill', 'inc', 'even', 'what', 'via', 'not', 'there', 'has', 'throughout', 'am', 'put', 'ie', 'computer', 'he', 'because', 'hence', 'due', 'top', 'say', 'without', 'around', 'using', 'within', 'therefore', 'else', 'whether', 'two', 'rather', 'themselves', 'six', 'however', 'a', 'less', 'along', 'every', 'kg', 'so', 'yet', 'per', 'five', 'already', 'down', 'other', 'side', 'unless', 'you', 'wherein', 'by', 'i', 'its', 'whereafter', 'neither', 'afterwards', 'all', 'found', 'go', 'various', 'above', 'last', 'amount', 'fill', 'yourself', 'km', 'ourselves', 'whatever', 'is', 'are', 'his', 'that', 'into', 'more', 'sometimes', 'have', 'becoming', 'together', 'became', 'then', 'being', 'enough', 'everywhere', 'twenty', 'name', 'those', 'myself', 'she', 'cant', 'thereby', 'towards', 'no', 'show', 'meanwhile', 'take', 'former', 'behind', 're', 'were', 'doesn', 'latterly', 'out', 'indeed', 'de', 'hereafter', 'whereby', 'thus', 'bottom', 'hundred', 'to', 'should', 'either', 'call', 'sixty', 'such', 'must', 'somewhere', 'did', 'detail', 'twelve', 'next', 'but', 'just', 'anything', 'ten', 'yours', 'had', 'beside', 'ltd', 'anyway', 'whole', 'except', 'quite', 'become', 'would', 'will', 'might', 'could', 'besides', 'and', 'toward', 'seem', 'on', 'eg', 'at', 'first', 'system', 'until', 'our', 'under', 'well', 'ours', 'own', 'whereupon', 'many', 'thereafter', 'everything', 'none', 'nevertheless', 'otherwise', 'un', 'seems', 'thick', 'beyond', 'really', 'how', 'ever', 'made', 'was', 'off', 'noone', 'nowhere', 'before', 'than', 'hers', 'serious', 'nine', 'whereas', 'therein', 'formerly', 'itself', 'never', 'few', 'one', 'who', 'below', 'latter', 'almost', 'where', 'whoever', 'whence', 'used', 'thin', 'nobody', 'others', 'thence', 'be', 'much', 'fifteen', 'co', 'get', 'anyone', 'namely', 'something', 'after', 'them', 'anyhow', 'whose', 'any', 'someone', 'doing', 'several', 'seemed', 'cry', 'why', 'with', 'see', 'we', 'your', 'both', 'eleven'})\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uowj9s6WBa-3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "f769b13e-3f77-461d-ac5a-b061733cb3fc"
      },
      "source": [
        "import re\n",
        "\n",
        "def remove_twitterisms(list_of_strings):\n",
        "    \"\"\"\n",
        "    Some regular expression statements to remove twitter-isms\n",
        "    \n",
        "    Operations:\n",
        "        - remove links\n",
        "        - remove @tag\n",
        "        - remove #tag\n",
        "        \n",
        "    Returns list of strings with the above removed\n",
        "    \"\"\"\n",
        "    \n",
        "    # removing some standard twitter-isms\n",
        "\n",
        "    list_of_strings = [re.sub(r\"http\\S+\", \"\", doc) for doc in list_of_strings]\n",
        "\n",
        "    list_of_strings = [re.sub(r\"@\\S+\", \"\", doc) for doc in list_of_strings]\n",
        "\n",
        "    list_of_strings = [re.sub(r\"#\\S+\", \"\", doc) for doc in list_of_strings]\n",
        "    \n",
        "    return list_of_strings"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i7eVcK7NBa-7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "044c3db6-0034-4172-9d4c-809a451aa804"
      },
      "source": [
        "# removing emojis\n",
        "# taken from https://gist.github.com/slowkow/7a7f61f495e3dbb7e3d767f97bd7304b#gistcomment-3315605\n",
        "\n",
        "def remove_emoji(string):\n",
        "    emoji_pattern = re.compile(\"[\"\n",
        "                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "                               u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
        "                               u\"\\U00002702-\\U000027B0\"\n",
        "                               u\"\\U00002702-\\U000027B0\"\n",
        "                               u\"\\U000024C2-\\U0001F251\"\n",
        "                               u\"\\U0001f926-\\U0001f937\"\n",
        "                               u\"\\U00010000-\\U0010ffff\"\n",
        "                               u\"\\u2640-\\u2642\"\n",
        "                               u\"\\u2600-\\u2B55\"\n",
        "                               u\"\\u200d\"\n",
        "                               u\"\\u23cf\"\n",
        "                               u\"\\u23e9\"\n",
        "                               u\"\\u231a\"\n",
        "                               u\"\\ufe0f\"  # dingbats\n",
        "                               u\"\\u3030\"\n",
        "                               \"]+\", flags=re.UNICODE)\n",
        "    return emoji_pattern.sub(r'', string)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "myz5QO3pBa_B",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "42ae7dae-c77d-4dcb-c822-cc75055cc41a"
      },
      "source": [
        "def remove_redacted(string):\n",
        "    \n",
        "    string = [re.sub(r\"(x|X){2,}\", \"\", doc) for doc in string]\n",
        "    \n",
        "    return string"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1gJda11XBa_Q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "009b148b-274a-4802-daac-1af83034e2de"
      },
      "source": [
        "from gensim.models.phrases import Phrases\n",
        "\n",
        "def n_gram(tokens):\n",
        "    \"\"\"Identifies common two/three word phrases using gensim module.\"\"\"\n",
        "    # Add bigrams and trigrams to docs (only ones that appear 10 times or more).\n",
        "    # includes threshold kwarg (threshold score required by bigram)\n",
        "    bigram = Phrases(tokens, min_count=10, threshold=100)\n",
        "    trigram = Phrases(bigram[tokens], threshold = 100)\n",
        "\n",
        "    for idx, val in enumerate(tokens):\n",
        "        for token in bigram[tokens[idx]]:\n",
        "            if '_' in token:\n",
        "                if token not in tokens[idx]:\n",
        "                    # Token is a bigram, add to document.bigram\n",
        "                    tokens[idx].append(token)\n",
        "        for token in trigram[tokens[idx]]:\n",
        "            if '_' in token:\n",
        "                if token not in tokens[idx]:\n",
        "                    # Token is a trigram, add to document.\n",
        "                    tokens[idx].append(token)\n",
        "    return tokens"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KaXXFqRCBa_V",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "f5ce3507-551a-4340-c2f7-e82aeaadd33c"
      },
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "def lemmatise(words):\n",
        "    \"\"\"\n",
        "    Convert words to their lemma or root using WordNet lemmatizer\n",
        "    \"\"\"\n",
        "    lemma = WordNetLemmatizer()\n",
        "    # this function takes a list of lists of tokens\n",
        "    return [[lemma.lemmatize(token,'v') for token in tokens] for tokens in words]"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tY1I8yJOBa_a",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "8e19c209-6de4-4f59-f802-74784f9fabbc"
      },
      "source": [
        "# next we implement the preprocessing steps\n",
        "\n",
        "preprocessed_corpus = remove_twitterisms(subsample_text)\n",
        "\n",
        "preprocessed_corpus = remove_redacted(preprocessed_corpus)\n",
        "\n",
        "preprocessed_corpus = [remove_emoji(doc) for doc in preprocessed_corpus]\n",
        "\n",
        "preprocessed_corpus = basic_preprocess(preprocessed_corpus)\n",
        "\n",
        "# added stemming\n",
        "preprocessed_corpus = lemmatise(preprocessed_corpus)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "error",
          "ename": "LookupError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     79\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{}/{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzip_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    672\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'\\n%s\\n%s\\n%s\\n'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 673\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    674\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mwordnet\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('wordnet')\n  \u001b[0m\n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/lib/nltk_data'\n**********************************************************************\n",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-fb1549033169>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# added stemming\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mpreprocessed_corpus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlemmatise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreprocessed_corpus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-16-7662de521b78>\u001b[0m in \u001b[0;36mlemmatise\u001b[0;34m(words)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mlemma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWordNetLemmatizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;31m# this function takes a list of lists of tokens\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlemma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlemmatize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'v'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtokens\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-16-7662de521b78>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mlemma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWordNetLemmatizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;31m# this function takes a list of lists of tokens\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlemma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlemmatize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'v'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtokens\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-16-7662de521b78>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mlemma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWordNetLemmatizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;31m# this function takes a list of lists of tokens\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlemma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlemmatize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'v'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtokens\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/nltk/stem/wordnet.py\u001b[0m in \u001b[0;36mlemmatize\u001b[0;34m(self, word, pos)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mlemmatize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNOUN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m         \u001b[0mlemmas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwordnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_morphy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlemmas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mlemmas\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m    114\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"LazyCorpusLoader object has no attribute '__bases__'\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m         \u001b[0;31m# This looks circular, but its not, since __load() changes our\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0;31m# __class__ to something new:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     79\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{}/{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzip_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m                 \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0;31m# Load the corpus.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m                 \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{}/{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{}/{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzip_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    671\u001b[0m     \u001b[0msep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'*'\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m70\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    672\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'\\n%s\\n%s\\n%s\\n'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 673\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    674\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    675\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mwordnet\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('wordnet')\n  \u001b[0m\n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/lib/nltk_data'\n**********************************************************************\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OX06PkBoBa_f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# lets compare the original strings to the preprocessed strings\n",
        "for orig, proc in zip(subsample_text[:5], preprocessed_corpus[:5]):\n",
        "    \n",
        "    print(orig)\n",
        "    print(proc)\n",
        "    print('\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IXU_tkIYBa_k",
        "colab_type": "text"
      },
      "source": [
        "## Building a bag of words corpus and dictionary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ri0B63CdBa_k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "from gensim.corpora import Dictionary\n",
        "\n",
        "def bag_of_word_processing(corpus_of_tokens, lower_extreme, upper_extreme):\n",
        "    \"\"\"\n",
        "    Take the list of tokens and convert them into a bag-of-words (BoW) format.\n",
        "\n",
        "    Extended description of function.\n",
        "\n",
        "    :param list of lists corpus_of_tokens: a list of strings produced during preprocessing representing all documents in corpus\n",
        "    :param int lower_extreme: Description of arg2.\n",
        "    :param float upper_extreme: the upper extreme filter limit, words are excluded if they occur in more documents than the proportion specified here\n",
        "    :return: gensim.corpora.dictionary.Dictionary object \n",
        "    :return: list representing BoW corpus\n",
        "    \n",
        "    \"\"\"\n",
        "\n",
        "    # Create a dictionary representation of the documents.\n",
        "    # gensim Dictionary function creates tokens -> tokenID dict\n",
        "    dictionary = Dictionary(corpus_of_tokens)\n",
        "    print('Number of unique words in initital documents:', len(dictionary))\n",
        "\n",
        "    org_dict = len(dictionary)\n",
        "\n",
        "    # Filter out words that occur less than 10 documents, or more than 70% of the documents.\n",
        "    dictionary.filter_extremes(no_below=lower_extreme, no_above=upper_extreme)\n",
        "    print('Number of unique words after removing rare and common words:', len(dictionary))\n",
        "\n",
        "    filt_dict = len(dictionary)\n",
        "\n",
        "    print('Token reduction of: ' + str((1-filt_dict/org_dict)*100)+'%')\n",
        "\n",
        "    # transform to bag of words\n",
        "    corpus = [dictionary.doc2bow(doc) for doc in corpus_of_tokens]\n",
        "    print('Number of unique tokens: %d' % len(dictionary))\n",
        "    print('Number of documents: %d' % len(corpus))\n",
        "\n",
        "    # output on document length\n",
        "    print('Average number of words per document before BoW transform: ', np.mean([len(item) for item in preprocessed_corpus]))\n",
        "    print('Average number of words per BoW document: ',np.mean([len(corpus[i]) for i in range(len(corpus))]))\n",
        "\n",
        "    return dictionary, corpus"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Swvk8xxBa_o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "working_dict, working_corpus = bag_of_word_processing(preprocessed_corpus, 10, 0.7)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ylG8DPLHBa_t",
        "colab_type": "text"
      },
      "source": [
        "## Building the topic model\n",
        "\n",
        "We now have processed our initial texts into components that machines can interact with and we're ready to create a topic model!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jneMHHcXBa_u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from gensim.models import CoherenceModel, ldamulticore\n",
        "\n",
        "def long_topic_scan(dictionary, corpus,  texts, limit, start=2, step=3):\n",
        "    \"\"\"\n",
        "    Identify the topic number with the highest coherence score out of a broad range of numbers\n",
        "    Adapted from https://www.machinelearningplus.com/nlp/topic-modeling-gensim-python/\n",
        "    Parameters:\n",
        "    ----------\n",
        "    dictionary : Gensim dictionary\n",
        "    corpus : Gensim corpus\n",
        "    texts : List of input texts (doc_clean)\n",
        "    limit : Max num of topics\n",
        "    start : int starting topic number\n",
        "    step : int increment from one topic number to another until limit is reached\n",
        "    Returns:\n",
        "    coherence_df : pd.DataFrame containing topic number and its calculated coherence score\n",
        "    -------\n",
        "    coherence_values : Coherence values corresponding to the LDA model with respective number of topics\n",
        "    graphical outputs\n",
        "    \"\"\"\n",
        "    coherence_dict = dict()\n",
        "\n",
        "    for num_topics in range(start, limit, step):\n",
        "        model = ldamulticore.LdaMulticore(corpus=corpus, num_topics=num_topics, id2word=dictionary, workers=2)\n",
        "        coherencemodel1 = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v')\n",
        "        coherence_dict[num_topics] = coherencemodel1.get_coherence()\n",
        "\n",
        "    coherence_df = pd.DataFrame(pd.Series(coherence_dict)).reset_index()\n",
        "\n",
        "    coherence_df.columns = ['Num_topics','Coherence_score']\n",
        "\n",
        "    # Show graph\n",
        "    fig, ax = plt.subplots(figsize=(12,10))\n",
        "    ax.plot(coherence_df['Num_topics'], coherence_df['Coherence_score'])\n",
        "    ax.set_xlabel(\"No. of topics\", fontweight='bold')\n",
        "    ax.set_ylabel(\"Cv Coherence score\", fontweight='bold')\n",
        "    ax.axvline(coherence_df[coherence_df['Coherence_score'] == coherence_df['Coherence_score'].max()]['Num_topics'].tolist(), color='red')\n",
        "\n",
        "    return coherence_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uj8yb0EOBa_z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%time\n",
        "\n",
        "coh_model = long_topic_scan(working_dict, working_corpus, preprocessed_corpus, limit=50, step=3, start=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ViQOvp7xBa_5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "coh_model.sort_values(ascending = False, by='Coherence_score').head(1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W0YsNr3RBa__",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# next we build our working model using the topic number we've determined\n",
        "\n",
        "working_model = ldamulticore.LdaMulticore(corpus=working_corpus, num_topics=5, id2word=working_dict, workers=2)\n",
        "\n",
        "coherencemodel1 = CoherenceModel(model=working_model, texts=preprocessed_corpus, dictionary=working_dict, coherence='c_v')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HFFgRIoxBbAC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "coherencemodel1.get_coherence()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UM0FAbiJBbAG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ideas about inspecting the model\n",
        "working_model.show_topics(-1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iMppLkFdNkxP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install pyLDAvis"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Surv1FdCBbAM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pyLDAvis.gensim\n",
        "\n",
        "pyLDAvis.enable_notebook()\n",
        "\n",
        "vis = pyLDAvis.gensim.prepare(working_model, working_corpus, dictionary=working_model.id2word)\n",
        "\n",
        "vis"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2GgTFbabBbAS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def format_topics_sentences(ldamodel, corpus, texts):\n",
        "    # Init output\n",
        "    sent_topics_df = pd.DataFrame()\n",
        "\n",
        "    # Get main topic in each document\n",
        "    # enumerate each topic and return number of topic, row of topic numbers and probabilities\n",
        "    for row in ldamodel[corpus]:\n",
        "        # sort row data into descending order\n",
        "        row = sorted(row, key=lambda x: (x[1]), reverse=True)\n",
        "        # Get the Dominant topic, Perc Contribution and Keywords for each document\n",
        "        # split row value into j (numerated), topic number, topic probability\n",
        "        # select top numerated (top ranked topic), retrieve topic text and join it altogether\n",
        "        # combine into pandas dataframe with topic text and probability of topic\n",
        "        for j, (topic_num, prop_topic) in enumerate(row):\n",
        "            if j == 0:  # => dominant topic\n",
        "                wordprob = ldamodel.show_topic(topic_num)\n",
        "                topic_keywords = \", \".join([word for word, prop in wordprob])\n",
        "                sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num) + 1,\n",
        "                                                                  round(prop_topic, 4),\n",
        "                                                                  topic_keywords]), ignore_index=True)\n",
        "            else:\n",
        "                break\n",
        "\n",
        "    # Add original text to the end of the output\n",
        "    contents = pd.Series(texts)\n",
        "    sent_topics_df = pd.concat([sent_topics_df, contents], axis=1)\n",
        "    # add column names\n",
        "    sent_topics_df.columns = ['Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords', 'Original text']\n",
        "\n",
        "    return sent_topics_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GOs_Pud5BbAX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "topic_df = format_topics_sentences(working_model, working_corpus, ticket_data[\"Consumer complaint narrative\"].tolist())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pkj_4LXBBbAa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "topic_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "czJgocHgBbAe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ticket_data.reset_index().join(topic_df).groupby('Product')['Dominant_Topic'].value_counts().unstack().plot.bar(figsize=(16,8))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oo-rE7evBbAh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_top3_docs(dominant_topic_frame):\n",
        "\n",
        "    table_lst = []\n",
        "\n",
        "    # create dataframe of top 3 most representative docs for each topic\n",
        "    for i in range(1, int(dominant_topic_frame['Dominant_Topic'].max())):\n",
        "        # get indexes\n",
        "        indy = dominant_topic_frame[dominant_topic_frame['Dominant_Topic'] == i].sort_values(by='Perc_Contribution', ascending=False).index.tolist()\n",
        "        # test how many documents passed\n",
        "        if len(indy) <= 3:\n",
        "            for idx in indy:\n",
        "                table_lst.append(dominant_topic_frame.iloc[idx, :])\n",
        "        else:\n",
        "            table_lst.append(dominant_topic_frame.iloc[indy[0], :])\n",
        "            table_lst.append(dominant_topic_frame.iloc[indy[1], :])\n",
        "            table_lst.append(dominant_topic_frame.iloc[indy[2], :])\n",
        "\n",
        "    new_eg_df = pd.DataFrame(table_lst)\n",
        "\n",
        "    return new_eg_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RZ_hs3l2BbAl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "top3_df = get_top3_docs(topic_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pv-ztcqZBbAp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "top3_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rgbpjGPKBbAv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for topic in top3_df.Dominant_Topic.unique():\n",
        "    \n",
        "    print(f\"Topic number {topic}\")\n",
        "    print(top3_df[top3_df.Dominant_Topic == topic].Topic_Keywords.tolist()[0], '\\n')\n",
        "    \n",
        "    subset_df = top3_df[top3_df.Dominant_Topic == topic]['Original text'].tolist()\n",
        "    \n",
        "    for idx, item in enumerate(subset_df):\n",
        "        print(\"text \", str(idx))\n",
        "        print(item, '\\n')\n",
        "    \n",
        "    print(\"----------\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KzwDp_1eBbA0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}