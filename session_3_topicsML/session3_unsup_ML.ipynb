{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Session 3: Text analysis approaches\n",
    "\n",
    "\\#\\#\\# __DRAFT__ \\#\\#\\#\n",
    "\n",
    "Text analysis is a classic computational and data science problem, \n",
    "\n",
    "Compared with regression and classification approaches on continuous and categorical dataset taking text data and deriving distinct insights is a far more complicated task. Text data and especially free text (text fields in sentence form) is typically classed as a form of unstructured data because of the various nuances introduced by languages.\n",
    "\n",
    "With the ever increasing computational power has come a side-by-side improvements in approaches to text analysis. \n",
    "\n",
    "The idea of topic modelling, identifying abstract 'topics' within a collection of documents (corpus) using statistical models, was first described in 1998, with probabilistic latent semantic analysis (PLSA) outlined in 1999 and latent Dirichlet allocation (LDA) developed in [2002](http://jmlr.csail.mit.edu/papers/v3/blei03a.html). LDA has become one of the most commonly used topic modelling approaches since although many extensions of LDA have since been proposed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gensim\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>author_id</th>\n",
       "      <th>inbound</th>\n",
       "      <th>created_at</th>\n",
       "      <th>text</th>\n",
       "      <th>response_tweet_id</th>\n",
       "      <th>in_response_to_tweet_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>sprintcare</td>\n",
       "      <td>False</td>\n",
       "      <td>Tue Oct 31 22:10:47 +0000 2017</td>\n",
       "      <td>@115712 I understand. I would like to assist y...</td>\n",
       "      <td>2</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>115712</td>\n",
       "      <td>True</td>\n",
       "      <td>Tue Oct 31 22:11:45 +0000 2017</td>\n",
       "      <td>@sprintcare and how do you propose we do that</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>115712</td>\n",
       "      <td>True</td>\n",
       "      <td>Tue Oct 31 22:08:27 +0000 2017</td>\n",
       "      <td>@sprintcare I have sent several private messag...</td>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>sprintcare</td>\n",
       "      <td>False</td>\n",
       "      <td>Tue Oct 31 21:54:49 +0000 2017</td>\n",
       "      <td>@115712 Please send us a Private Message so th...</td>\n",
       "      <td>3</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>115712</td>\n",
       "      <td>True</td>\n",
       "      <td>Tue Oct 31 21:49:35 +0000 2017</td>\n",
       "      <td>@sprintcare I did.</td>\n",
       "      <td>4</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   tweet_id   author_id  inbound                      created_at  \\\n",
       "0         1  sprintcare    False  Tue Oct 31 22:10:47 +0000 2017   \n",
       "1         2      115712     True  Tue Oct 31 22:11:45 +0000 2017   \n",
       "2         3      115712     True  Tue Oct 31 22:08:27 +0000 2017   \n",
       "3         4  sprintcare    False  Tue Oct 31 21:54:49 +0000 2017   \n",
       "4         5      115712     True  Tue Oct 31 21:49:35 +0000 2017   \n",
       "\n",
       "                                                text response_tweet_id  \\\n",
       "0  @115712 I understand. I would like to assist y...                 2   \n",
       "1      @sprintcare and how do you propose we do that               NaN   \n",
       "2  @sprintcare I have sent several private messag...                 1   \n",
       "3  @115712 Please send us a Private Message so th...                 3   \n",
       "4                                 @sprintcare I did.                 4   \n",
       "\n",
       "   in_response_to_tweet_id  \n",
       "0                      3.0  \n",
       "1                      1.0  \n",
       "2                      4.0  \n",
       "3                      5.0  \n",
       "4                      6.0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import the dataset\n",
    "\n",
    "twitter_data = pd.read_csv('data/twcs.csv')\n",
    "\n",
    "twitter_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>author_id</th>\n",
       "      <th>inbound</th>\n",
       "      <th>created_at</th>\n",
       "      <th>text</th>\n",
       "      <th>response_tweet_id</th>\n",
       "      <th>in_response_to_tweet_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>115712</td>\n",
       "      <td>True</td>\n",
       "      <td>Tue Oct 31 22:11:45 +0000 2017</td>\n",
       "      <td>@sprintcare and how do you propose we do that</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>115712</td>\n",
       "      <td>True</td>\n",
       "      <td>Tue Oct 31 22:08:27 +0000 2017</td>\n",
       "      <td>@sprintcare I have sent several private messag...</td>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>115712</td>\n",
       "      <td>True</td>\n",
       "      <td>Tue Oct 31 21:49:35 +0000 2017</td>\n",
       "      <td>@sprintcare I did.</td>\n",
       "      <td>4</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>8</td>\n",
       "      <td>115712</td>\n",
       "      <td>True</td>\n",
       "      <td>Tue Oct 31 21:45:10 +0000 2017</td>\n",
       "      <td>@sprintcare is the worst customer service</td>\n",
       "      <td>9,6,10</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>12</td>\n",
       "      <td>115713</td>\n",
       "      <td>True</td>\n",
       "      <td>Tue Oct 31 22:04:47 +0000 2017</td>\n",
       "      <td>@sprintcare You gonna magically change your co...</td>\n",
       "      <td>11,13,14</td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   tweet_id author_id  inbound                      created_at  \\\n",
       "1         2    115712     True  Tue Oct 31 22:11:45 +0000 2017   \n",
       "2         3    115712     True  Tue Oct 31 22:08:27 +0000 2017   \n",
       "4         5    115712     True  Tue Oct 31 21:49:35 +0000 2017   \n",
       "6         8    115712     True  Tue Oct 31 21:45:10 +0000 2017   \n",
       "8        12    115713     True  Tue Oct 31 22:04:47 +0000 2017   \n",
       "\n",
       "                                                text response_tweet_id  \\\n",
       "1      @sprintcare and how do you propose we do that               NaN   \n",
       "2  @sprintcare I have sent several private messag...                 1   \n",
       "4                                 @sprintcare I did.                 4   \n",
       "6          @sprintcare is the worst customer service            9,6,10   \n",
       "8  @sprintcare You gonna magically change your co...          11,13,14   \n",
       "\n",
       "   in_response_to_tweet_id  \n",
       "1                      1.0  \n",
       "2                      4.0  \n",
       "4                      6.0  \n",
       "6                      NaN  \n",
       "8                     15.0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a sub dataframe for just data that was inbound\n",
    "inbound_dat = twitter_data[twitter_data.inbound == True]\n",
    "\n",
    "inbound_dat.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(702669,)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inbound_dat.author_id.unique().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join together all inbound tweets from the same user\n",
    "\n",
    "user_tweets = inbound_dat.groupby('author_id')['text'].apply(lambda x: ','.join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Screw you @116016 and your stupid Blueprint program. I never signed up for this crap and now you’re going to charge me interest fees? https://t.co/WwBzUIhSbG,@ChaseSupport Actually it just doesn’t work in Safari, but that’s still pretty bad.,Dear @ChaseSupport, it’s kinda hard to pay my bills when the entire payment section of your site is unavailable 🤦🏻\\u200d♀️',\n",
       " \"Now the flight @Delta is sending our bag back on just got delayed two hours. So mad right now, I can't even.\",\n",
       " '@MOO Big thanks to Quentin for the exceptional service! Just ordered our 3rd round of #businesscards 👍,The ribotRainbow! New #businesscards thanks to @moo 😊#rainbow #ourteamrocks https://t.co/nqMMUqYzKt https://t.co/gVtJDEoGFu',\n",
       " 'Yup https://t.co/GpkFa9MfHQ,same. https://t.co/gxkJt8BNV6',\n",
       " '@comcastcares Is it possible to get business class internet at a residence, and if so are there any restrictions/limitations?',\n",
       " '@Delta I just sent you a DM,@Delta I will never fly your airline again',\n",
       " 'Wow. Used to think Apple maps were bad. Just took a 30 minute 10 mile @115879 ride to a destination that was two miles away. #walking would have been faster. #fail',\n",
       " '@AmericanAir Thank you! The info provided isn’t new nor helpful. Can you get him his bag within 24 hrs? He needs his medications.  #RuinedVacation,@AmericanAir Just sent. He’s been without his scuba gear  for four days and has run out of his medication. Please help!,My sweetie has been stranded in Honduras without his bags for FOUR DAYS. No wifi or cell. @AmericanAir a little help? #RuinedVacation,@AmericanAir Thank you! When can we expect the next update?,@AmericanAir Any update? Five days and counting. Is the bag delayed or lost at this point? #RuinedVacation,@AmericanAir Thank you! That update didn’t include any new info nor any plan to get more info. Would love to hear one or both of those. #RuinedVacation,@AmericanAir If you could let us know specific steps you’re taking and the timeline for taking them, that would be far more helpful &amp; build more trust.,@AmericanAir Thank you! We have been hearing that exact thing for four days, however, so I’m sure you can understand why that answer isn’t satisfying.,@AmericanAir Thank you! When can we expect an update? Time is of the essence.,Ok, brain trust! My sweetie is in Honduras for a diving trip, but @AmericanAir has kept his bag w/his meds and gear in NY for 4 days. Ideas?',\n",
       " '@AppleSupport thanks, sent you a DM,Every iOS 11 update has slowed my phone down. GYST @115858',\n",
       " '@Uber_Support Done. \\n\\nI went ahead and drove to the restaurant to pick up my food myself. https://t.co/KcSB5uwsyT,@115877 My food was supposed to be delivered an hour ago. Still not here. Been on hold with your customer support for 10 minutes. No one answering. #HANGRY https://t.co/WsHSLxZIMJ']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_tweets = user_tweets.reset_index()\n",
    "\n",
    "user_tweets.text[:10].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "Pre processing is a crucial step in any text analytics project. Text data on its own is very difficult for machines to understand and therefore it requires cleaning and preparing before building models. This often involves a number of steps such as:\n",
    "- Tokenisation, converting a long string of words into a list of individual words i.e. \"the cat sat on the mat\" -> [\"the\", \"cat\", \"sat\", \"on\", \"the\", \"mat\"]\n",
    "- Noise removal, most commonly removing punctuation or things like hyperlinks or emojis\n",
    "- Stopword removal, removing common words that don't contain information such as the, and, or, a \n",
    "- Stemming or lemming, this is the process of reverting words to their root either by chopping off suffixes (stemming) or reverting to word lemma (lemming)\n",
    "- Normalisation, commonly this means converting all words to lower or uppercase\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.parsing.preprocessing import preprocess_string, strip_tags, strip_punctuation, strip_numeric, remove_stopwords\n",
    "\n",
    "def basic_preprocess(list_of_strings):\n",
    "    \"\"\"\n",
    "    A basic function that takes a list of strings and runs some basic\n",
    "    gensim preprocessing to tokenise each string.\n",
    "    \n",
    "    Operations:\n",
    "        - convert to lowercase\n",
    "        - remove html tags\n",
    "        - remove punctuation\n",
    "        - remove numbers\n",
    "    \n",
    "    Outputs a list of lists\n",
    "    \"\"\"\n",
    "    \n",
    "    CUSTOM_FILTERS = [lambda x: x.lower(), strip_tags, strip_punctuation, strip_numeric, remove_stopwords]\n",
    "\n",
    "    preproc_text = [preprocess_string(doc, CUSTOM_FILTERS) for doc in list_of_strings]\n",
    "    \n",
    "    return preproc_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frozenset({'anyway', 'seems', 'unless', 'de', 'am', 'however', 'himself', 'others', 'off', 'put', 'through', 'last', 'yet', 'only', 'across', 'serious', 'mostly', 'during', 'becomes', 'is', 'before', 'three', 'i', 'then', 'yourselves', 'did', 'hundred', 'until', 'say', 'more', 'at', 'don', 'per', 'empty', 'already', 'further', 'under', 'kg', 'cant', 'behind', 'whence', 'moreover', 'do', 'if', 'became', 'anything', 'where', 'up', 'but', 'when', 'several', 'via', 'down', 'though', 'below', 'while', 'against', 'than', 'even', 'elsewhere', 'alone', 'together', 'regarding', 'she', 'whatever', 'a', 'thereafter', 'with', 'are', 'either', 'its', 'take', 'may', 'also', 'you', 'else', 'amount', 'will', 'none', 'inc', 'front', 'now', 'fifteen', 'toward', 'seem', 'top', 'doing', 'towards', 'beyond', 'part', 'own', 'latter', 'whereafter', 'anywhere', 'name', 'must', 'namely', 'themselves', 'mine', 'whether', 'of', 'should', 'sixty', 'noone', 'used', 'bottom', 'nevertheless', 'here', 'itself', 'really', 'forty', 'our', 'since', 'therein', 'all', 'see', 'computer', 'quite', 'done', 'whither', 'eight', 'sincere', 'two', 'after', 'couldnt', 'myself', 'somewhere', 'amoungst', 'every', 'afterwards', 'call', 'us', 'and', 'still', 'fire', 'neither', 'twelve', 'perhaps', 'never', 'can', 'mill', 'much', 'ourselves', 'cannot', 'had', 'etc', 'show', 'move', 'does', 'wherever', 'been', 'just', 'please', 'make', 'un', 'among', 'those', 'any', 'nowhere', 'thereupon', 'have', 'therefore', 'no', 'wherein', 'along', 'get', 'many', 'these', 'nothing', 'on', 'why', 'most', 'both', 'might', 'my', 'seeming', 'hereupon', 'thick', 'becoming', 'without', 'seemed', 'indeed', 'anyone', 'detail', 'give', 'go', 'enough', 'they', 'hereafter', 'this', 'nor', 'first', 'beside', 'as', 'which', 'rather', 'has', 'km', 'too', 'from', 'thus', 'next', 'upon', 'latterly', 'somehow', 'four', 'ten', 'found', 'throughout', 'onto', 'always', 'hereby', 'everyone', 'someone', 'con', 'few', 'hers', 'nine', 'side', 'for', 'didn', 'herself', 'around', 'well', 'find', 'or', 'twenty', 'ie', 'her', 'except', 'so', 're', 'the', 'although', 'his', 'whose', 'whole', 'between', 'him', 'ours', 'over', 'often', 'being', 'formerly', 'whenever', 'who', 'full', 'be', 'co', 'your', 'eleven', 'less', 'sometimes', 'cry', 'he', 'them', 'yours', 'would', 'whom', 'something', 'once', 'out', 'nobody', 'what', 'beforehand', 'another', 'within', 'bill', 'hasnt', 'thereby', 'thru', 'almost', 'an', 'meanwhile', 'other', 'ever', 'amongst', 'due', 'by', 'thin', 'eg', 'become', 'former', 'made', 'in', 'everywhere', 'herein', 'everything', 'could', 'to', 'their', 'we', 'fifty', 'were', 'it', 'otherwise', 'ltd', 'each', 'sometime', 'besides', 'again', 'system', 'various', 'five', 'fill', 'not', 'least', 'whereby', 'very', 'whereupon', 'keep', 'some', 'interest', 'into', 'six', 'was', 'hence', 'back', 'one', 'using', 'above', 'third', 'yourself', 'about', 'anyhow', 'whoever', 'how', 'whereas', 'me', 'because', 'same', 'describe', 'doesn', 'thence', 'such', 'there', 'that'})\n"
     ]
    }
   ],
   "source": [
    "# what are stop words?\n",
    "\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "\n",
    "print(STOPWORDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def remove_twitterisms(list_of_strings):\n",
    "    \"\"\"\n",
    "    Some regular expression statements to remove twitter-isms\n",
    "    \n",
    "    Operations:\n",
    "        - remove links\n",
    "        - remove @tag\n",
    "        - remove #tag\n",
    "        \n",
    "    Returns list of strings with the above removed\n",
    "    \"\"\"\n",
    "    \n",
    "    # removing some standard twitter-isms\n",
    "\n",
    "    list_of_strings = [re.sub(r\"http\\S+\", \"\", doc) for doc in list_of_strings]\n",
    "\n",
    "    list_of_strings = [re.sub(r\"@\\S+\", \"\", doc) for doc in list_of_strings]\n",
    "\n",
    "    list_of_strings = [re.sub(r\"#\\S+\", \"\", doc) for doc in list_of_strings]\n",
    "    \n",
    "    return list_of_strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing emojis\n",
    "# taken from https://gist.github.com/slowkow/7a7f61f495e3dbb7e3d767f97bd7304b#gistcomment-3315605\n",
    "\n",
    "def remove_emoji(string):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                               u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
    "                               u\"\\U00002702-\\U000027B0\"\n",
    "                               u\"\\U00002702-\\U000027B0\"\n",
    "                               u\"\\U000024C2-\\U0001F251\"\n",
    "                               u\"\\U0001f926-\\U0001f937\"\n",
    "                               u\"\\U00010000-\\U0010ffff\"\n",
    "                               u\"\\u2640-\\u2642\"\n",
    "                               u\"\\u2600-\\u2B55\"\n",
    "                               u\"\\u200d\"\n",
    "                               u\"\\u23cf\"\n",
    "                               u\"\\u23e9\"\n",
    "                               u\"\\u231a\"\n",
    "                               u\"\\ufe0f\"  # dingbats\n",
    "                               u\"\\u3030\"\n",
    "                               \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.phrases import Phrases\n",
    "\n",
    "def n_gram(tokens):\n",
    "    \"\"\"Identifies common two/three word phrases using gensim module.\"\"\"\n",
    "    # Add bigrams and trigrams to docs (only ones that appear 10 times or more).\n",
    "    # includes threshold kwarg (threshold score required by bigram)\n",
    "    bigram = Phrases(tokens, min_count=10, threshold=100)\n",
    "    trigram = Phrases(bigram[tokens], threshold = 100)\n",
    "\n",
    "    for idx, val in enumerate(tokens):\n",
    "        for token in bigram[tokens[idx]]:\n",
    "            if '_' in token:\n",
    "                if token not in tokens[idx]:\n",
    "                    # Token is a bigram, add to document.bigram\n",
    "                    tokens[idx].append(token)\n",
    "        for token in trigram[tokens[idx]]:\n",
    "            if '_' in token:\n",
    "                if token not in tokens[idx]:\n",
    "                    # Token is a trigram, add to document.\n",
    "                    tokens[idx].append(token)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def test_basic_proprocess():\n",
    "    \n",
    "    # just to randomise tests\n",
    "    randomize = random.randrange(0,3)\n",
    "    \n",
    "    test_doc = [\n",
    "        \"THE CAT SAT ON THE MAT!\",\n",
    "        \"<h1> hello world?</h1>\",\n",
    "        \"wElL WeLl We?L, wHaT <strong>HaVe wE</strong>hERe\"\n",
    "    ]\n",
    "    \n",
    "    test_output = basic_preprocess(test_doc)\n",
    "    \n",
    "    # test if func returns list of lists\n",
    "    assert isinstance(test_output[randomize], list)\n",
    "    # test if all items \n",
    "    assert [x.islower() for x in test_output[randomize]] == [True] * len(test_output[randomize])\n",
    "    # test if html tags are removed\n",
    "    assert \"<h1>\" not in test_output[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# next we implement the preprocessing steps\n",
    "\n",
    "preprocessed_corpus = remove_twitterisms(user_tweets.text.tolist())\n",
    "\n",
    "preprocessed_corpus = [remove_emoji(doc) for doc in preprocessed_corpus]\n",
    "\n",
    "preprocessed_corpus = basic_preprocess(preprocessed_corpus)\n",
    "\n",
    "preprocessed_corpus = n_gram(preprocessed_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Screw you @116016 and your stupid Blueprint program. I never signed up for this crap and now you’re going to charge me interest fees? https://t.co/WwBzUIhSbG,@ChaseSupport Actually it just doesn’t work in Safari, but that’s still pretty bad.,Dear @ChaseSupport, it’s kinda hard to pay my bills when the entire payment section of your site is unavailable 🤦🏻‍♀️\n",
      "['screw', 'stupid', 'blueprint', 'program', 'signed', 'crap', 'you’re', 'going', 'charge', 'fees', 'actually', 'doesn’t', 'work', 'safari', 'that’s', 'pretty', 'bad', 'dear', 'it’s', 'kinda', 'hard', 'pay', 'bills', 'entire', 'payment', 'section', 'site', 'unavailable']\n",
      "\n",
      "\n",
      "Now the flight @Delta is sending our bag back on just got delayed two hours. So mad right now, I can't even.\n",
      "['flight', 'sending', 'bag', 'got', 'delayed', 'hours', 'mad', 'right', 't']\n",
      "\n",
      "\n",
      "@MOO Big thanks to Quentin for the exceptional service! Just ordered our 3rd round of #businesscards 👍,The ribotRainbow! New #businesscards thanks to @moo 😊#rainbow #ourteamrocks https://t.co/nqMMUqYzKt https://t.co/gVtJDEoGFu\n",
      "['big', 'thanks', 'quentin', 'exceptional', 'service', 'ordered', 'rd', 'round', 'ribotrainbow', 'new', 'thanks']\n",
      "\n",
      "\n",
      "Yup https://t.co/GpkFa9MfHQ,same. https://t.co/gxkJt8BNV6\n",
      "['yup']\n",
      "\n",
      "\n",
      "@comcastcares Is it possible to get business class internet at a residence, and if so are there any restrictions/limitations?\n",
      "['possible', 'business', 'class', 'internet', 'residence', 'restrictions', 'limitations']\n",
      "\n",
      "\n",
      "@Delta I just sent you a DM,@Delta I will never fly your airline again\n",
      "['sent', 'dm', 'fly', 'airline']\n",
      "\n",
      "\n",
      "Wow. Used to think Apple maps were bad. Just took a 30 minute 10 mile @115879 ride to a destination that was two miles away. #walking would have been faster. #fail\n",
      "['wow', 'think', 'apple', 'maps', 'bad', 'took', 'minute', 'mile', 'ride', 'destination', 'miles', 'away', 'faster']\n",
      "\n",
      "\n",
      "@AmericanAir Thank you! The info provided isn’t new nor helpful. Can you get him his bag within 24 hrs? He needs his medications.  #RuinedVacation,@AmericanAir Just sent. He’s been without his scuba gear  for four days and has run out of his medication. Please help!,My sweetie has been stranded in Honduras without his bags for FOUR DAYS. No wifi or cell. @AmericanAir a little help? #RuinedVacation,@AmericanAir Thank you! When can we expect the next update?,@AmericanAir Any update? Five days and counting. Is the bag delayed or lost at this point? #RuinedVacation,@AmericanAir Thank you! That update didn’t include any new info nor any plan to get more info. Would love to hear one or both of those. #RuinedVacation,@AmericanAir If you could let us know specific steps you’re taking and the timeline for taking them, that would be far more helpful &amp; build more trust.,@AmericanAir Thank you! We have been hearing that exact thing for four days, however, so I’m sure you can understand why that answer isn’t satisfying.,@AmericanAir Thank you! When can we expect an update? Time is of the essence.,Ok, brain trust! My sweetie is in Honduras for a diving trip, but @AmericanAir has kept his bag w/his meds and gear in NY for 4 days. Ideas?\n",
      "['thank', 'info', 'provided', 'isn’t', 'new', 'helpful', 'bag', 'hrs', 'needs', 'medications', 'sent', 'he’s', 'scuba', 'gear', 'days', 'run', 'medication', 'help', 'sweetie', 'stranded', 'honduras', 'bags', 'days', 'wifi', 'cell', 'little', 'help', 'thank', 'expect', 'update', 'update', 'days', 'counting', 'bag', 'delayed', 'lost', 'point', 'thank', 'update', 'didn’t', 'include', 'new', 'info', 'plan', 'info', 'love', 'hear', 'let', 'know', 'specific', 'steps', 'you’re', 'taking', 'timeline', 'taking', 'far', 'helpful', 'amp', 'build', 'trust', 'thank', 'hearing', 'exact', 'thing', 'days', 'i’m', 'sure', 'understand', 'answer', 'isn’t', 'satisfying', 'thank', 'expect', 'update', 'time', 'essence', 'ok', 'brain', 'trust', 'sweetie', 'honduras', 'diving', 'trip', 'kept', 'bag', 'w', 'meds', 'gear', 'ny', 'days', 'ideas']\n",
      "\n",
      "\n",
      "@AppleSupport thanks, sent you a DM,Every iOS 11 update has slowed my phone down. GYST @115858\n",
      "['thanks', 'sent', 'dm', 'ios', 'update', 'slowed', 'phone', 'gyst']\n",
      "\n",
      "\n",
      "@Uber_Support Done. \n",
      "\n",
      "I went ahead and drove to the restaurant to pick up my food myself. https://t.co/KcSB5uwsyT,@115877 My food was supposed to be delivered an hour ago. Still not here. Been on hold with your customer support for 10 minutes. No one answering. #HANGRY https://t.co/WsHSLxZIMJ\n",
      "['went', 'ahead', 'drove', 'restaurant', 'pick', 'food', 'food', 'supposed', 'delivered', 'hour', 'ago', 'hold', 'customer', 'support', 'minutes', 'answering']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# lets compare the original strings to the preprocessed strings\n",
    "for orig, proc in zip(user_tweets.text.tolist()[:10], preprocessed_corpus[:10]):\n",
    "    \n",
    "    print(orig)\n",
    "    print(proc)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a bag of words corpus and dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the topic model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
